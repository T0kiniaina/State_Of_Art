{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/T0kiniaina/State_Of_Art/blob/main/State_of_Art_code.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EICOsRGt1RAj"
      },
      "outputs": [],
      "source": [
        "!pip install scholarly"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import random\n",
        "from scholarly import scholarly\n",
        "import pandas as pd\n",
        "\n",
        "# Define your search query\n",
        "query = \"machine learning\"\n",
        "\n",
        "# Search Google Scholar\n",
        "search_query = scholarly.search_pubs(query)\n",
        "\n",
        "# Initialize an empty list to store the results\n",
        "results = []\n",
        "\n",
        "# Iterate through the search results\n",
        "for i in range(3):  # Get the top 10 results\n",
        "    try:\n",
        "        paper = next(search_query)\n",
        "\n",
        "        # Extraer los campos relevantes del paper\n",
        "        title = paper.get('bib', {}).get('title', 'Unknown')\n",
        "        author = paper.get('bib', {}).get('author', 'Unknown')\n",
        "        year = paper.get('bib', {}).get('pub_year', 'Unknown')\n",
        "        citations = paper.get('num_citations', 'Unknown')\n",
        "        doi = paper.get('pub_url', 'Unknown')  # DOI usualmente está en 'pub_url'\n",
        "        institution = paper.get('source', 'Unknown')  # Fuente o institución de publicación\n",
        "\n",
        "        # Append the result\n",
        "        results.append({\n",
        "            \"title\": title,\n",
        "            \"author\": author,\n",
        "            \"year\": year,\n",
        "            \"citations\": citations,\n",
        "            \"doi\": doi,\n",
        "            \"institution\": institution\n",
        "        })\n",
        "\n",
        "        # Añadir un retardo aleatorio entre 5 y 15 segundos\n",
        "        sleep_time = random.uniform(5, 15)\n",
        "        print(f\"Sleeping for {sleep_time:.2f} seconds\")\n",
        "        time.sleep(sleep_time)\n",
        "\n",
        "    except StopIteration:\n",
        "        break\n",
        "\n",
        "# Create a pandas DataFrame from the results\n",
        "df = pd.DataFrame(results)\n",
        "\n",
        "# Print the DataFrame\n",
        "df\n"
      ],
      "metadata": {
        "id": "TwwUz2oL78a5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import random\n",
        "from scholarly import scholarly\n",
        "import pandas as pd\n",
        "\n",
        "# Define your search query\n",
        "query = \"machine learning\"\n",
        "\n",
        "# Search Google Scholar\n",
        "search_query = scholarly.search_pubs(query)\n",
        "\n",
        "# Initialize an empty list to store the results\n",
        "results = []\n",
        "\n",
        "# Iterate through the search results\n",
        "for i in range(3):  # Get the top 10 results\n",
        "    try:\n",
        "        paper = next(search_query)\n",
        "\n",
        "        # Extraer los campos relevantes del paper\n",
        "        title = paper.get('bib', {}).get('title', 'Unknown')\n",
        "        authors = paper.get('bib', {}).get('author', 'Unknown')\n",
        "        year = paper.get('bib', {}).get('pub_year', 'Unknown')\n",
        "        citations = paper.get('num_citations', 'Unknown')\n",
        "        doi = paper.get('pub_url', 'Unknown')  # DOI o URL al paper\n",
        "        journal = paper.get('bib', {}).get('journal', 'Unknown')  # Revista o conferencia\n",
        "        abstract = paper.get('bib', {}).get('abstract', 'Unknown')  # Resumen del paper\n",
        "        keywords = paper.get('bib', {}).get('keywords', 'Unknown')  # Palabras clave (si están disponibles)\n",
        "        institution = paper.get('source', 'Unknown')  # Fuente o institución de publicación\n",
        "        pdf_url = paper.get('eprint_url', doi)  # Enlace al texto completo si está disponible\n",
        "\n",
        "        # Append the result\n",
        "        results.append({\n",
        "            \"title\": title,\n",
        "            \"authors\": authors,\n",
        "            \"year\": year,\n",
        "            \"citations\": citations,\n",
        "            \"doi\": doi,\n",
        "            # \"journal/conference\": journal,\n",
        "            \"abstract\": abstract,\n",
        "            # \"keywords\": keywords,\n",
        "            # \"institution\": institution,\n",
        "            # \"pdf_url\": pdf_url\n",
        "        })\n",
        "\n",
        "        # Añadir un retardo aleatorio entre 5 y 15 segundos\n",
        "        sleep_time = random.uniform(5, 15)\n",
        "        print(f\"Sleeping for {sleep_time:.2f} seconds\")\n",
        "        time.sleep(sleep_time)\n",
        "\n",
        "    except StopIteration:\n",
        "        break\n",
        "\n",
        "# Create a pandas DataFrame from the results\n",
        "df = pd.DataFrame(results)\n",
        "\n",
        "# Print the DataFrame\n",
        "df\n"
      ],
      "metadata": {
        "id": "we7zKeb883Qy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install scholarly"
      ],
      "metadata": {
        "id": "BxpPBV781VcJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import random\n",
        "from scholarly import scholarly\n",
        "import pandas as pd\n",
        "from tenacity import retry, stop_after_attempt, wait_random  # For retry mechanism\n",
        "import itertools  # To cycle through proxies\n",
        "\n",
        "# Retry configuration: retry up to 3 times with random wait between 5 and 10 seconds\n",
        "@retry(stop=stop_after_attempt(3), wait=wait_random(min=5, max=10))\n",
        "def fetch_paper(paper, proxy):\n",
        "    # Extract relevant fields from the paper\n",
        "    title = paper.get('bib', {}).get('title', 'Unknown')\n",
        "    authors = paper.get('bib', {}).get('author', 'Unknown')\n",
        "    year = paper.get('bib', {}).get('pub_year', 'Unknown')\n",
        "    citations = paper.get('num_citations', 'Unknown')\n",
        "    doi = paper.get('pub_url', 'Unknown')  # DOI or URL for the paper\n",
        "    institution = paper.get('source', 'Unknown')  # Source or institution\n",
        "    abstract = paper.get('bib', {}).get('abstract', 'Unknown')\n",
        "    journal = paper.get('bib', {}).get('venue', 'Unknown')\n",
        "    volume = paper.get('bib', {}).get('volume', 'Unknown')\n",
        "    issue = paper.get('bib', {}).get('issue', 'Unknown')\n",
        "    pages = paper.get('bib', {}).get('pages', 'Unknown')\n",
        "\n",
        "    return {\n",
        "        \"Title\": title,\n",
        "        \"Author(s)\": authors,\n",
        "        \"Year\": year,\n",
        "        \"Citations\": citations,\n",
        "        \"DOI\": doi,\n",
        "        \"Institution\": institution,\n",
        "        \"Abstract\": abstract,\n",
        "        \"Journal/Conference Name\": journal,\n",
        "        \"Volume\": volume,\n",
        "        \"Issue\": issue,\n",
        "        \"Pages\": pages,\n",
        "        \"Proxy Used\": proxy  # Track the proxy used\n",
        "    }\n",
        "\n",
        "# Define a list of proxies\n",
        "proxies = [\n",
        "    {\"http\": \"http://123.45.67.89:8080\", \"https\": \"http://123.45.67.89:8080\"},  # Replace with real IP\n",
        "    {\"http\": \"http://98.76.54.32:3128\", \"https\": \"http://98.76.54.32:3128\"},    # Replace with real IP\n",
        "    {\"http\": \"http://11.22.33.44:80\", \"https\": \"http://11.22.33.44:80\"},        # Replace with real IP\n",
        "]\n",
        "\n",
        "# Cycle through proxies\n",
        "proxy_cycle = itertools.cycle(proxies)\n",
        "\n",
        "# Define the list of search queries (keywords)\n",
        "keywords = [\"Image processing\", \"Thermal Image\", \"GDP\"]\n",
        "\n",
        "# Initialize an empty list to store the results\n",
        "results = []\n",
        "\n",
        "# Loop through each keyword\n",
        "for keyword in keywords:\n",
        "    print(f\"Searching for keyword: {keyword}\")\n",
        "    search_query = scholarly.search_pubs(keyword)  # Update the search query for each keyword\n",
        "\n",
        "    # Fetch up to 10 results for each keyword with proxy rotation\n",
        "    for i in range(3):\n",
        "        try:\n",
        "            paper = next(search_query)\n",
        "            current_proxy = next(proxy_cycle)  # Get the next proxy from the cycle\n",
        "            result = fetch_paper(paper, current_proxy)  # Fetch and retry if needed, with the current proxy\n",
        "            result[\"Keyword\"] = keyword  # Add the keyword to the result\n",
        "            results.append(result)\n",
        "\n",
        "            # Random sleep to avoid rate limiting (can be reduced if needed)\n",
        "            sleep_time = random.uniform(3, 5)\n",
        "            print(f\"Sleeping for {sleep_time:.2f} seconds, using proxy: {current_proxy}\")\n",
        "            time.sleep(sleep_time)\n",
        "\n",
        "        except StopIteration:\n",
        "            break\n",
        "\n",
        "# Create a pandas DataFrame from the results\n",
        "df = pd.DataFrame(results)\n",
        "\n",
        "# Print the DataFrame or save it to a CSV file\n",
        "# df.to_csv('google_scholar_results_with_proxies_keywords.csv', index=False)\n",
        "# print(df)\n",
        "df"
      ],
      "metadata": {
        "id": "Gv-41ZzIwUI6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Save the DataFrame to your Google Drive folder\n",
        "folder_path = '/content/drive/My Drive/Data_analytics/Projects/20241008_state_of_art/'  # Replace 'your_folder_name' with the actual folder name in your Drive\n",
        "file_name = 'google_scholar.csv'\n",
        "file_path = folder_path + file_name\n",
        "\n",
        "df.to_csv(file_path, index=False, encoding='utf-8-sig')\n",
        "print(f\"File saved to: {file_path}\")"
      ],
      "metadata": {
        "id": "HxBC3NvN6FOh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install git"
      ],
      "metadata": {
        "id": "FsBRE-0pNtC7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git config --global user.email \"frodrigueza.career@example.com\"\n",
        "!git config --global user.name \"Mike-R0d\""
      ],
      "metadata": {
        "id": "oDyj14OaNvdZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/Mike-R0d/State_of_Art.git"
      ],
      "metadata": {
        "id": "5JwErkRbOJhp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd State_of_Art\n"
      ],
      "metadata": {
        "id": "cks_wPfVOOGJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the dataframe in the GitHub folder\n",
        "file_path = '/content/State_of_Art/google_scholar_results.csv'\n",
        "df.to_csv(file_path, index=False)\n"
      ],
      "metadata": {
        "id": "exqo3FdcOXFh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}